import torch
import torch.nn as nn
import os
from torch.utils.data import DataLoader
from transformers import ASTFeatureExtractor, GPT2Tokenizer
from dataset import CWRUDataset
from model import SignalCaptioningModel

# --- AYARLAR ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    print(f"ğŸš€ Cuda aktif! Cihaz: {torch.cuda.get_device_name(0)}")

EPOCHS = 20          # Uzun soluklu, hassas eÄŸitim
BATCH_SIZE = 4
LEARNING_RATE = 1e-5 # DÄ°KKAT: HÄ±zÄ± dÃ¼ÅŸÃ¼rdÃ¼k (Fine-tuning iÃ§in ÅŸart)
PATIENCE = 5         

print(f"Ä°leri Seviye EÄŸitim (Fine-Tuning) {device} Ã¼zerinde baÅŸlÄ±yor...")

# --- HAZIRLIK ---
feature_extractor = ASTFeatureExtractor.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Dataset (RAM Cache)
print("Veri seti hazÄ±rlanÄ±yor...")
dataset = CWRUDataset('data/raw', feature_extractor)
train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Modeli BaÅŸlat
model = SignalCaptioningModel("MIT/ast-finetuned-audioset-10-10-0.4593", "gpt2").to(device)

# --- KRÄ°TÄ°K HAMLE 1: ESKÄ° MODELÄ° YÃœKLE ---
best_model_path = "motor_model_best.pth"
if os.path.exists(best_model_path):
    print(f"ğŸ“¥ Ã–nceki ÅŸampiyon model yÃ¼kleniyor: {best_model_path}")
    model.load_state_dict(torch.load(best_model_path, map_location=device))
    print("âœ… KaldÄ±ÄŸÄ±mÄ±z yerden devam ediyoruz!")
else:
    print("âš ï¸ KayÄ±tlÄ± model bulunamadÄ±, sÄ±fÄ±rdan baÅŸlanÄ±yor (Ã–nerilmez).")

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

# --- KRÄ°TÄ°K HAMLE 2: SCHEDULER (AKILLI HIZ KONTROLÃœ) ---
# DÃœZELTME: 'verbose=True' kaldÄ±rÄ±ldÄ± (Yeni PyTorch sÃ¼rÃ¼mlerinde hata veriyor)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)

# Takip DeÄŸiÅŸkenleri
best_loss = float('inf') 
patience_counter = 0     

model.train()

print("\nğŸ”¥ Hassas EÄŸitim BaÅŸlÄ±yor! Hedef: ArÄ±za BoyutlarÄ±nÄ± Ã–ÄŸretmek.\n")

for epoch in range(EPOCHS):
    total_loss = 0
    
    # MEVCUT HIZI BÄ°Z ELLE YAZDIRALIM (Verbose yerine)
    current_lr = optimizer.param_groups[0]['lr']
    print(f"--- Epoch {epoch+1}/{EPOCHS} BaÅŸladÄ± (LR: {current_lr:.8f}) ---")
    
    for batch_idx, (signals, labels) in enumerate(train_loader):
        signals = signals.to(device)
        targets = tokenizer(labels, 
                            return_tensors="pt", 
                            padding="max_length", 
                            max_length=20, 
                            truncation=True).input_ids.to(device)
        
        optimizer.zero_grad()
        outputs = model(signals, labels=targets)
        loss = outputs.loss
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 20 == 0: 
            print(f"Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}")

    # Epoch sonu
    avg_loss = total_loss / len(train_loader)
    print(f"ğŸ Epoch {epoch+1} Bitti. Ort. KayÄ±p: {avg_loss:.4f}")

    # --- SCHEDULER GÃœNCELLEME ---
    # Loss durumuna gÃ¶re hÄ±zÄ± ayarla (Arka planda Ã§alÄ±ÅŸÄ±r)
    scheduler.step(avg_loss)
    
    # HÄ±z dÃ¼ÅŸtÃ¼ mÃ¼ diye kontrol edelim
    new_lr = optimizer.param_groups[0]['lr']
    if new_lr < current_lr:
        print(f"ğŸ“‰ DÄ°KKAT: Ã–ÄŸrenme hÄ±zÄ± dÃ¼ÅŸÃ¼rÃ¼ldÃ¼! ({current_lr:.8f} -> {new_lr:.8f})")

    # --- KAYIT MANTIÄI ---
    if avg_loss < best_loss:
        best_loss = avg_loss
        patience_counter = 0
        torch.save(model.state_dict(), "motor_model_best_tuned.pth") 
        print(f"ğŸ’¾ DAHA Ä°YÄ°SÄ° BULUNDU! Kaydedildi: motor_model_best_tuned.pth (Loss: {best_loss:.4f})")
    else:
        patience_counter += 1
        print(f"â³ Ä°yileÅŸme yok. SabÄ±r: {patience_counter}/{PATIENCE}")
        
        if patience_counter >= PATIENCE:
            print(f"\nğŸ›‘ ERKEN DURDURMA! Model artÄ±k limitlerine ulaÅŸtÄ±.")
            break

print("\nâœ… Hassas EÄŸitim TamamlandÄ±!")